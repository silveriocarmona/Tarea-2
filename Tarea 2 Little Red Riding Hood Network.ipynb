{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Jessica Barón Martínez Código: 200924758 \n",
    "# Silverio Carmona Código: 200924758 \n",
    "\n",
    "\n",
    "\n",
    "## Exercise 05\n",
    "\n",
    "# Neural networks\n",
    "\n",
    "## 4.1 Little Red Riding Hood Network\n",
    "\n",
    "Train a neural network to solve the  Little Red Riding Hood problem in sklern and Keras. Try the neural networ with different inputs and report the results.\n",
    "\n",
    "________________\n",
    "\n",
    "## 4.2 Boston House Price Prediction\n",
    "\n",
    "In the next questions we are going to work using the dataset *Boston*. This dataset measures the influence of socioeconomical factors on the price of several estates of the city of Boston. This dataset has 506 instances, each one characterized by 13 features:\n",
    "\n",
    "* CRIM - per capita crime rate by town\n",
    "* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS - proportion of non-retail business acres per town.\n",
    "* CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX - nitric oxides concentration (parts per 10 million)\n",
    "* RM - average number of rooms per dwelling\n",
    "* AGE - proportion of owner-occupied units built prior to 1940\n",
    "* DIS - weighted distances to five Boston employment centres\n",
    "* RAD - index of accessibility to radial highways\n",
    "* TAX - full-value property-tax rate per 10,000 USD\n",
    "* PTRATIO - pupil-teacher ratio by town\n",
    "* B - $1000(Bk - 0.63)^2$ where $Bk$ is the proportion of blacks by town\n",
    "* LSTAT - % lower status of the population\n",
    "\n",
    "Output variable:\n",
    "* MEDV - Median value of owner-occupied homes in 1000's USD\n",
    "\n",
    "**Note:** In this exercise we are going to predict the price of each estate, which is represented in the `MEDV` variable. It is important to remember that we are always aiming to predict `MEDV`, no matter which explanatory variables we are using. That means, in some cases we will use a subset of the 13 previously mentioned variables, while in other cases we will use all the 13 variables. But in no case we will change the dependent variable $y$.\n",
    "\n",
    "\n",
    "\n",
    "1. Load the dataset using `from sklearn.datasets import load_boston`.\n",
    "2. Create a DataFrame using the attribute `.data` from the loading function of Scikit-learn.\n",
    "3. Assign the columns of the DataFrame so they match the `.feature_names` attribute from the loading function of Scikit-learn. \n",
    "4. Assign a new column to the DataFrame which holds the value to predict, that means, the `.target` attribute of the loading function of Scikit-learn. The name of this columns must be `MEDV`.\n",
    "5. Use the function `.describe()` from Pandas for obtaining statistics about each column.\n",
    "\n",
    "## 4.3 Feature analysis:\n",
    "\n",
    "Using the DataFrame generated in the previous section:\n",
    "* Filter the dataset to just these features:\n",
    "     * Explanatory: 'LSTAT', 'INDUS', 'NOX', 'RM', 'AGE'\n",
    "     * Dependent: 'MEDV'.\n",
    "* Generate a scatter matrix among the features mentioned above using Pandas (`scatter_matrix`) or Seaborn (` pairplot`).\n",
    "     * Do you find any relationship between the features?\n",
    "* Generate the correlation matrix between these variables using `numpy.corrcoef`. Also include `MEDV`.\n",
    "     * Which characteristics are more correlated?\n",
    "     * BONUS: Visualize this matrix as heat map using Pandas, Matplotlib or Seaborn.\n",
    "\n",
    "## 4.4 Modeling linear and non linear relationships\n",
    "\n",
    "* Generate two new subsets filtering these characteristics:\n",
    "     * $D_1$:  $X = \\textit{'RM'}$, $y = \\textit{'MEDV'}$\n",
    "     * $D_2$:  $X = \\textit{'LSTAT'}$, $y = \\textit{'MEDV'}$\n",
    "* For each subset, generate a training partition and a test partition using a ratio of $ 70 \\% - 30 \\% $\n",
    "* Train a linear regression model on both subsets of data:\n",
    "     * Report the mean square error on the test set\n",
    "     * Print the values of $ w $ and $ w_0 $ of the regression equation\n",
    "     * Generate a graph where you visualize the line obtained by the regression model in conjunction with the training data and the test data\n",
    "* How does the model perform on $ D_1 $ and $ D_2 $? Why?\n",
    "\n",
    "## 4.5 Training a regression model\n",
    "\n",
    "* Generate a 70-30 partitioning of the data **using all the features**. (Do not include the dependent variable `MEDV`)\n",
    "* Train a linear regression model with the objective of predicting the output variable `MEDV`.\n",
    "     * Report the mean square error on the test set\n",
    "* Train a regression model using `MLPRegressor` in order to predict the output variable` MEDV`.\n",
    "     * Report the mean square error on the test set\n",
    "* Scale the data so that they have zero mean variance one per feature (only $ X $). You can use the following piece of code:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc_x = StandardScaler()\n",
    "sc_x.fit(X)\n",
    "X_train_s = sc_x.transform(X_train)\n",
    "X_test_s = sc_x.transform(X_test)\n",
    "```\n",
    "Check more information about `StandardScaler` [here](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "\n",
    "* Train the following models:\n",
    "     1. Train a linear regression model using the scaled data.\n",
    "         * Report the mean square error on the test set\n",
    "     2. Train a regression model using a 2-layer MultiLayer Perceptron (128 neurons in the first and 512 in the second) and with the **scaled data**.\n",
    "         * Report the mean square error on the test set\n",
    "     3. Which model has better performance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUCIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Little Red Riding Hood Network\n",
    "Train a neural network to solve the Little Red Riding Hood problem in sklern and Keras. Try the neural networ with different inputs and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from sklearn.datasets.samples_generator import make_moons\n",
    "import keras\n",
    "from keras import models\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural network training in Keras\n",
    "We will build a multilayer network to solve the following classification problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos las 3 combinaciones par identiicar personaje\n",
    "\n",
    "training_data=np.array([[1,1,0,0],[0,1,1,0],[0,0,0,1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos las 3 combinaciones que se obtienen par identificar personaje\n",
    "target_data=np.array([[1,0,0,0],[0,0,1,1],[0,1,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0],\n",
       "       [0, 1, 1, 0],\n",
       "       [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 1, 1],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINICIÓN DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(12, input_dim=4, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x234dd2d85c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPILACIÓN DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#elige automáticamente la mejor manera de representar la red para entrenar y hacer predicciones\n",
    "#Se utiliza la pérdida logarítmica, que para un problema de clasificación binaria se define en Keras como “binary_crossentropy”.\n",
    "#algoritmo de descenso de gradiente eficiente “adam” por su alta eficiencia.\n",
    "#Debido a que es un problema de clasificación, recopilaremos y reportaremos la exactitud de la clasificación como la métrica \"accuracy\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                60        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 173\n",
      "Trainable params: 173\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Podemos comprobar nuestra arquitectura de modelo con el siguiente comando\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENTRENAMIENTO DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "model.fit(x=training_data,y=target_data, epochs=150, batch_size=1)\n",
    "\n",
    "#El proceso de entrenamiento se ejecutará para un número fijo de iteraciones denominado epochs o épocas.\n",
    "#el número de instancias que se evalúan antes de que se realice una actualización de peso en la red llamada batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUACIÓN DEL MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(training_data, target_data)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDECIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# round predictions\n",
    "rounded = [round(x[0]) for x in predictions]\n",
    "print(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
